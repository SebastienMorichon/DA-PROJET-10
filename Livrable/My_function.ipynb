{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "063f8561",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Sommaire<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Exploration-des-données\" data-toc-modified-id=\"Exploration-des-données-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Exploration des données</a></span><ul class=\"toc-item\"><li><span><a href=\"#my_exploration()\" data-toc-modified-id=\"my_exploration()-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>my_exploration()</a></span></li><li><span><a href=\"#my_valeurs_manquantes()\" data-toc-modified-id=\"my_valeurs_manquantes()-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>my_valeurs_manquantes()</a></span></li><li><span><a href=\"#my_first_analyse()\" data-toc-modified-id=\"my_first_analyse()-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>my_first_analyse()</a></span></li><li><span><a href=\"#my_data_visualisation()\" data-toc-modified-id=\"my_data_visualisation()-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>my_data_visualisation()</a></span></li></ul></li><li><span><a href=\"#Nettoyage-des-données\" data-toc-modified-id=\"Nettoyage-des-données-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Nettoyage des données</a></span><ul class=\"toc-item\"><li><span><a href=\"#my_missing_values()\" data-toc-modified-id=\"my_missing_values()-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>my_missing_values()</a></span></li><li><span><a href=\"#my_fill_missing_data()\" data-toc-modified-id=\"my_fill_missing_data()-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>my_fill_missing_data()</a></span></li><li><span><a href=\"#my_data_scaler()\" data-toc-modified-id=\"my_data_scaler()-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>my_data_scaler()</a></span></li><li><span><a href=\"#My_inverse_log\" data-toc-modified-id=\"My_inverse_log-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>My_inverse_log</a></span></li></ul></li><li><span><a href=\"#Analyse-Exploratoire-de-Données\" data-toc-modified-id=\"Analyse-Exploratoire-de-Données-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Analyse Exploratoire de Données</a></span><ul class=\"toc-item\"><li><span><a href=\"#Analyse-Univériée\" data-toc-modified-id=\"Analyse-Univériée-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Analyse Univériée</a></span><ul class=\"toc-item\"><li><span><a href=\"#my_boxplots()\" data-toc-modified-id=\"my_boxplots()-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>my_boxplots()</a></span></li><li><span><a href=\"#my_outliers_zscore()\" data-toc-modified-id=\"my_outliers_zscore()-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>my_outliers_zscore()</a></span></li></ul></li><li><span><a href=\"#Analyse-Bivariée\" data-toc-modified-id=\"Analyse-Bivariée-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Analyse Bivariée</a></span><ul class=\"toc-item\"><li><span><a href=\"#my_corr_heatmap()\" data-toc-modified-id=\"my_corr_heatmap()-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>my_corr_heatmap()</a></span></li></ul></li><li><span><a href=\"#Tests-statistique\" data-toc-modified-id=\"Tests-statistique-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Tests statistique</a></span><ul class=\"toc-item\"><li><span><a href=\"#my_pearson()\" data-toc-modified-id=\"my_pearson()-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>my_pearson()</a></span></li></ul></li><li><span><a href=\"#Projection-des-individus\" data-toc-modified-id=\"Projection-des-individus-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Projection des individus</a></span><ul class=\"toc-item\"><li><span><a href=\"#my_projection_individus()\" data-toc-modified-id=\"my_projection_individus()-3.4.1\"><span class=\"toc-item-num\">3.4.1&nbsp;&nbsp;</span>my_projection_individus()</a></span></li></ul></li><li><span><a href=\"#Analyse-en-Composante-Principales-(ACP-ou-PCA)\" data-toc-modified-id=\"Analyse-en-Composante-Principales-(ACP-ou-PCA)-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Analyse en Composante Principales (ACP ou PCA)</a></span><ul class=\"toc-item\"><li><span><a href=\"#my_pca_coude()\" data-toc-modified-id=\"my_pca_coude()-3.5.1\"><span class=\"toc-item-num\">3.5.1&nbsp;&nbsp;</span>my_pca_coude()</a></span></li><li><span><a href=\"#my_pca()\" data-toc-modified-id=\"my_pca()-3.5.2\"><span class=\"toc-item-num\">3.5.2&nbsp;&nbsp;</span>my_pca()</a></span></li></ul></li><li><span><a href=\"#KMeans\" data-toc-modified-id=\"KMeans-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>KMeans</a></span><ul class=\"toc-item\"><li><span><a href=\"#my_kmeans_coude()\" data-toc-modified-id=\"my_kmeans_coude()-3.6.1\"><span class=\"toc-item-num\">3.6.1&nbsp;&nbsp;</span>my_kmeans_coude()</a></span></li><li><span><a href=\"#my_silhouette_method()\" data-toc-modified-id=\"my_silhouette_method()-3.6.2\"><span class=\"toc-item-num\">3.6.2&nbsp;&nbsp;</span>my_silhouette_method()</a></span></li><li><span><a href=\"#my_kmeans()\" data-toc-modified-id=\"my_kmeans()-3.6.3\"><span class=\"toc-item-num\">3.6.3&nbsp;&nbsp;</span>my_kmeans()</a></span></li><li><span><a href=\"#my_all_kmeans()\" data-toc-modified-id=\"my_all_kmeans()-3.6.4\"><span class=\"toc-item-num\">3.6.4&nbsp;&nbsp;</span>my_all_kmeans()</a></span></li></ul></li><li><span><a href=\"#Dendrogramme\" data-toc-modified-id=\"Dendrogramme-3.7\"><span class=\"toc-item-num\">3.7&nbsp;&nbsp;</span>Dendrogramme</a></span><ul class=\"toc-item\"><li><span><a href=\"#my_dendrogram()\" data-toc-modified-id=\"my_dendrogram()-3.7.1\"><span class=\"toc-item-num\">3.7.1&nbsp;&nbsp;</span>my_dendrogram()</a></span></li></ul></li><li><span><a href=\"#PCA-+-Projection-des-individus\" data-toc-modified-id=\"PCA-+-Projection-des-individus-3.8\"><span class=\"toc-item-num\">3.8&nbsp;&nbsp;</span>PCA + Projection des individus</a></span><ul class=\"toc-item\"><li><span><a href=\"#my_pca_proj()\" data-toc-modified-id=\"my_pca_proj()-3.8.1\"><span class=\"toc-item-num\">3.8.1&nbsp;&nbsp;</span>my_pca_proj()</a></span></li></ul></li></ul></li><li><span><a href=\"#Machine-Learning\" data-toc-modified-id=\"Machine-Learning-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Machine Learning</a></span><ul class=\"toc-item\"><li><span><a href=\"#my_backward_selected()\" data-toc-modified-id=\"my_backward_selected()-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>my_backward_selected()</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87f6ebf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing, decomposition\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c1d5b0",
   "metadata": {},
   "source": [
    "# Exploration des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ff8eb8",
   "metadata": {},
   "source": [
    "## my_exploration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d90a1a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_exploration(data):\n",
    "    display(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6eb4fd",
   "metadata": {},
   "source": [
    "## my_valeurs_manquantes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99f024e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_valeurs_manquantes(data):\n",
    "    #Affichage des % de valeurs manquantes par colonnes.\n",
    "    print(\"Affichage en % des valeurs manquantes par colonnes\")\n",
    "    tableValeurManquante = pd.DataFrame(((data.isna().sum()/data.shape[0])*100).round(2).sort_values(ascending=True))\n",
    "    tableValeurManquante[\"Nb Valeurs Manquantes\"]=data.isna().sum()\n",
    "    display(tableValeurManquante)\n",
    "    #Affichage graphique\n",
    "    plt.figure(figsize=(20,20))\n",
    "    sns.heatmap(data.isna(), cbar=None)\n",
    "    plt.title(\"Représentation graphique des valeurs manquantes\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5fb0c8",
   "metadata": {},
   "source": [
    "## my_first_analyse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81336b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_first_analyse(data, graphique=False):\n",
    "    \"\"\"\n",
    "    Effectue une analyse exploratoire des données et renvoie un résumé statistique.\n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame contenant les données à analyser.\n",
    "    Returns:\n",
    "        summary (pandas.DataFrame): DataFrame contenant un résumé statistique des données.\n",
    "    \"\"\"\n",
    "    # Calcul du nombre d'observations et de variables\n",
    "    nb_observations, nb_variables = data.shape\n",
    "    \n",
    "    # Calcul des statistiques descriptives\n",
    "    moyennes = data.mean()\n",
    "    medianes = data.median()\n",
    "    ecart_types = data.std()\n",
    "    mini = data.min()\n",
    "    maxi = data.max()\n",
    "    nb_valeurs_manquantes = data.isnull().sum()\n",
    "    data_type = data.dtypes\n",
    "    \n",
    "    # Création du DataFrame de résumé statistique\n",
    "    summary = pd.DataFrame({\n",
    "        'observations': nb_observations,\n",
    "        'variables': nb_variables,\n",
    "        'type' : data_type,\n",
    "        'moyennes': moyennes,\n",
    "        'medianes': medianes,\n",
    "        'ecart_types': ecart_types,\n",
    "        'min': mini,\n",
    "        'max': maxi,\n",
    "        'nb_valeurs_manquantes': nb_valeurs_manquantes,\n",
    "        '%_valeurs_manquantes' : round((nb_valeurs_manquantes/nb_observations)*100,2)\n",
    "    })\n",
    "    if graphique:\n",
    "        #Affichage graphique\n",
    "        plt.figure(figsize=(20,20))\n",
    "        sns.heatmap(data.isna(), cbar=None)\n",
    "        plt.title(\"Représentation graphique des valeurs manquantes\")\n",
    "        plt.show()\n",
    "    \n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdd2127",
   "metadata": {},
   "source": [
    "## my_data_visualisation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98d19f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_data_visualisation(df, variable):\n",
    "    \"\"\"\n",
    "    Crée un histogramme de la variable spécifiée.\n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame contenant les données à visualiser.\n",
    "        variable (str): Nom de la variable à visualiser.\n",
    "    \"\"\"\n",
    "    df[variable].hist()\n",
    "    plt.title(variable)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c01cfe4",
   "metadata": {},
   "source": [
    "# Nettoyage des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c457ef",
   "metadata": {},
   "source": [
    "## my_missing_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bb917dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_missing_values(df, apply_all=False, strategie=None):\n",
    "    \"\"\"\n",
    "    Traite les données manquantes en fonction de la stratégie spécifiée par l'utilisateur pour chaque colonne.\n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame contenant les données à traiter.\n",
    "        apply_all (bool): Si True, applique la même stratégie de traitement pour toutes les colonnes contenant des données manquantes. Sinon, demande à l'utilisateur de choisir une stratégie pour chaque colonne.\n",
    "    Returns:\n",
    "        df (pandas.DataFrame): DataFrame contenant les données traitées.\n",
    "    \"\"\"\n",
    "    strategies = {}\n",
    "    \n",
    "    # Vérifier si toutes les colonnes doivent avoir la même stratégie\n",
    "    if apply_all:\n",
    "        strat = strategie\n",
    "        if strat == 'valeur_plus_frequente':\n",
    "            for col in df.columns[df.isna().any()].tolist():\n",
    "                valeur_plus_frequente = df[col].value_counts().idxmax()\n",
    "                df[col].fillna(valeur_plus_frequente, inplace=True)\n",
    "        elif strat == 'valeur_constante':\n",
    "            constante = input(\"Entrez une valeur constante pour remplacer les données manquantes : \")\n",
    "            for col in df.columns[df.isna().any()].tolist():\n",
    "                df[col].fillna(constante, inplace=True)\n",
    "        elif strat == 'supprimer':\n",
    "            df = df.dropna()\n",
    "        elif strat == 'remplacer_par_moyenne':\n",
    "            df.fillna(df.mean(), inplace=True)\n",
    "        elif strat == 'remplacer_par_mediane':\n",
    "            df.fillna(df.median(), inplace=True)\n",
    "    else:\n",
    "        # Demander à l'utilisateur de choisir une stratégie pour chaque colonne contenant des données manquantes\n",
    "        for col in df.columns[df.isna().any()].tolist():\n",
    "            strat = input(f\"Choisissez une stratégie de traitement pour la colonne '{col}':\\n - 'supprimer' : supprime les lignes contenant des données manquantes\\n - 'remplacer_par_moyenne' : remplace les données manquantes par la moyenne de la colonne\\n - 'remplacer_par_mediane' : remplace les données manquantes par la médiane de la colonne\\n - 'valeur_plus_frequente' : remplace les données manquantes par la valeur la plus fréquente de la colonne\\n - 'valeur_constante' : remplace les données manquantes par une valeur constante\\n\")\n",
    "            strategies[col] = strat\n",
    "\n",
    "        # Appliquer les stratégies de traitement choisies\n",
    "        for col, strat in strategies.items():\n",
    "            if strat == 'supprimer':\n",
    "                df = df.dropna(subset=[col])\n",
    "            elif strat == 'remplacer_par_moyenne':\n",
    "                moyenne = df[col].mean()\n",
    "                df[col].fillna(moyenne, inplace=True)\n",
    "            elif strat == 'remplacer_par_mediane':\n",
    "                mediane = df[col].median()\n",
    "                df[col].fillna(mediane, inplace=True)\n",
    "            elif strat == 'valeur_plus_frequente':\n",
    "                valeur = df[col].mode().iloc[0]\n",
    "                df[col].fillna(valeur, inplace=True)\n",
    "            elif strat == 'valeur_constante':\n",
    "                valeur_constante = input(f\"Entrez une valeur constante pour remplacer les données manquantes de la colonne '{col}': \")\n",
    "                df[col].fillna(valeur_constante, inplace=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360f94a5",
   "metadata": {},
   "source": [
    "## my_fill_missing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7db2e871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_fill_missing_data(data, method='linear', axis=0):\n",
    "    \"\"\"\n",
    "    Remplit les données manquantes dans un tableau avec la méthode spécifiée.\n",
    "    :param data: Un tableau numpy avec des données manquantes.\n",
    "    :param method: La méthode à utiliser pour remplir les données manquantes.\n",
    "                   Les valeurs acceptées sont : 'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'.\n",
    "                   La valeur par défaut est 'linear'.\n",
    "    :param axis: L'axe pour remplir les données manquantes.\n",
    "                 Si axis = 0 (par défaut), les données manquantes sont remplacées par colonne.\n",
    "                 Si axis = 1, les données manquantes sont remplacées par ligne.\n",
    "    :return: Un tableau numpy avec les données manquantes remplacées.\n",
    "    \"\"\"\n",
    "    # Vérifier que la valeur de l'axe est correcte\n",
    "    if axis != 0 and axis != 1:\n",
    "        raise ValueError(\"L'axe doit être soit 0, soit 1.\")\n",
    "\n",
    "    # Trouver les indices des données manquantes\n",
    "    missing = np.isnan(data)\n",
    "    \n",
    "\n",
    "    # Si toutes les valeurs sont manquantes, retourner le tableau original\n",
    "    if np.all(missing):\n",
    "        return data\n",
    "\n",
    "    # Si aucune valeur n'est manquante, retourner le tableau original\n",
    "    if not np.any(missing):\n",
    "        return data\n",
    "\n",
    "    # Transposer le tableau si l'axe est 1\n",
    "    if axis == 1:\n",
    "        data = data.T\n",
    "        missing = missing.T\n",
    "\n",
    "    # Trouver les indices des données non manquantes\n",
    "    not_missing = np.logical_not(missing)\n",
    "\n",
    "    # Séparer les données manquantes et non manquantes\n",
    "    missing_x = np.argwhere(missing)\n",
    "    missing_y = data[not_missing]\n",
    "    not_missing_x = np.argwhere(not_missing)\n",
    "    not_missing_y = data[not_missing]\n",
    "\n",
    "    # Appliquer la méthode de remplissage\n",
    "    if method == 'linear':\n",
    "        model = LinearRegression()\n",
    "        model.fit(not_missing_x, not_missing_y)\n",
    "        missing_y = model.predict(missing_x)\n",
    "    elif method == 'nearest':\n",
    "        missing_y = np.interp(missing_x, not_missing_x, not_missing_y)\n",
    "    elif method == 'zero':\n",
    "        missing_y = np.zeros(missing_x.shape)\n",
    "    elif method == 'slinear':\n",
    "        missing_y = np.interp(missing_x, not_missing_x, not_missing_y, left=0, right=0)\n",
    "    elif method == 'quadratic':\n",
    "        p = np.polyfit(not_missing_x.flatten(), not_missing_y, 2)\n",
    "        missing_y = np.polyval(p, missing_x.flatten())\n",
    "    elif method == 'cubic':\n",
    "        p = np.polyfit(not_missing_x.flatten(), not_missing_y, 3)\n",
    "        missing_y = np.polyval(p, missing_x.flatten())\n",
    "\n",
    "    # Insérer les données manquantes dans le tableau\n",
    "    data[missing] = missing_y.flatten()\n",
    "\n",
    "    # Transposer le tableau si l'axe est 1\n",
    "    if axis == 1:\n",
    "        data = data.T\n",
    "        \n",
    "    print(missing)\n",
    "    print(data)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf3db8e",
   "metadata": {},
   "source": [
    "## my_data_scaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f496365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def my_data_scaler(df, columns=None, method=\"standard\", scaler_all=False):\n",
    "    \"\"\"\n",
    "    Scale les données d'un DataFrame en utilisant différentes méthodes de scaling.\n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame contenant les données à scaler.\n",
    "        columns (list): Liste des noms de colonnes à scaler. Si aucune liste n'est spécifiée, toutes les colonnes sont scalées (par défaut: None).\n",
    "        method (str): Méthode de scaling à utiliser. Les valeurs possibles sont \"standard\" (par défaut), \"minmax\", \"robust\" et \"log\".\n",
    "        scaler_all (bool): Si True, scaler toutes les colonnes du DataFrame (par défaut: False).\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame contenant les données scalées.\n",
    "    \"\"\"\n",
    "    # On vérifie si on doit appliquer le scaler sur toutes nos données\n",
    "    if columns is None and not scaler_all:\n",
    "        raise ValueError(\"Spécifiez les colonnes à scaler ou activez l'option 'scaler_all' pour scaler toutes les colonnes.\")\n",
    "    \n",
    "    if scaler_all:\n",
    "        columns = df.columns.tolist()\n",
    "    \n",
    "    # On sélectionne la méthode de Scalage et on l'applique à nos données.\n",
    "    if method == \"standard\":\n",
    "        scaler = StandardScaler()\n",
    "    elif method == \"minmax\":\n",
    "        scaler = MinMaxScaler()\n",
    "    elif method == \"robust\":\n",
    "        scaler = RobustScaler()\n",
    "    elif method == \"log\":\n",
    "        def log_scaler(data):\n",
    "            return np.log1p(data)\n",
    "        scaler = log_scaler\n",
    "    else:\n",
    "        raise ValueError(\"Méthode de scaling non valide. Les valeurs possibles sont 'standard', 'minmax', 'robust' et 'log'.\")\n",
    "\n",
    "    # On scale nos données avec la méthode sélectionnée    \n",
    "    if method == \"log\":\n",
    "        df_scaled = df[columns].apply(scaler)\n",
    "    else:\n",
    "        scaler.fit(df[columns])\n",
    "        scaled_data = scaler.transform(df[columns])\n",
    "        df_scaled = pd.DataFrame(scaled_data, columns=columns, index=df.index)\n",
    "    \n",
    "    # On remplace nos données initiales par les données scalées.\n",
    "    df = pd.concat([df.drop(columns, axis=1), df_scaled], axis=1)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878642f5",
   "metadata": {},
   "source": [
    "## My_inverse_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50307769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_inverse_log(df, columns=None, inverse_log_all=False):\n",
    "    \"\"\"\n",
    "    Effectue l'inverse du log sur les données d'un DataFrame.\n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame contenant les données.\n",
    "        columns (list): Liste des noms de colonnes sur lesquelles appliquer l'inverse du log. Si aucune liste n'est spécifiée, toutes les colonnes sont traitées (par défaut: None).\n",
    "        inverse_log_all (bool): Si True, appliquer l'inverse du log sur toutes les colonnes du DataFrame (par défaut: False).\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame contenant les données avec l'inverse du log appliqué.\n",
    "    \"\"\"\n",
    "    # Vérifier si on doit appliquer l'inverse du log sur toutes les colonnes\n",
    "    if columns is None and not inverse_log_all:\n",
    "        raise ValueError(\"Spécifiez les colonnes sur lesquelles appliquer l'inverse du log ou activez l'option 'inverse_log_all' pour toutes les colonnes.\")\n",
    "    \n",
    "    if inverse_log_all:\n",
    "        columns = df.columns.tolist()\n",
    "    \n",
    "    # Appliquer l'inverse du log sur les colonnes sélectionnées\n",
    "    df_inverse_log = df[columns].apply(lambda x: np.expm1(x))\n",
    "    \n",
    "    # Remplacer les données initiales par les données avec l'inverse du log appliqué\n",
    "    df_result = pd.concat([df.drop(columns, axis=1), df_inverse_log], axis=1)\n",
    "    \n",
    "    return df_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e021ea",
   "metadata": {},
   "source": [
    "# Analyse Exploratoire de Données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210e9975",
   "metadata": {},
   "source": [
    "## Analyse Univériée"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dfd220",
   "metadata": {},
   "source": [
    "### my_boxplots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32ee32c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_boxplots(df):\n",
    "    \"\"\"\n",
    "    Crée un graphique boxplot pour chaque variable d'un DataFrame.\n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame contenant les données.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(15,10))\n",
    "    df.boxplot(ax=ax)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3988d1d9",
   "metadata": {},
   "source": [
    "### my_outliers_zscore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fd87b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_outliers_zscore(df, seuil=3):\n",
    "    \"\"\"\n",
    "    Cette fonction prend en entrée un DataFrame et utilise la méthode Z-Score pour détecter les valeurs aberrantes.\n",
    "    Elle retourne un nouveau DataFrame contenant toutes les valeurs aberrantes.\n",
    "    \"\"\"\n",
    "    data = df\n",
    "    \n",
    "    # Calcule le Z-Score pour chaque colonne du DataFrame\n",
    "    z_scores = stats.zscore(data)\n",
    "\n",
    "    # Trouve toutes les valeurs dont la valeur absolue du Z-Score est supérieure à 3 (seuil de détection)\n",
    "    outliers = data[(abs(z_scores) > seuil).any(axis=1)]\n",
    "\n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb321404",
   "metadata": {},
   "source": [
    "## Analyse Bivariée"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed9cc32",
   "metadata": {},
   "source": [
    "### my_corr_heatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2fd6419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_corr_heatmap(data, annot=True):\n",
    "    \"\"\"\n",
    "    Calcule le coefficient de corrélation de Pearson entre toutes les paires de variables d'un DataFrame.\n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame contenant les données.\n",
    "    Returns:\n",
    "        sns.heatmap: heatmap contenant les coefficients de corrélation entre toutes les paires de variables.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.title(\"Heatmap de corrélation de Pearson\")\n",
    "    sns.heatmap(data.corr(), annot=annot,cmap=\"coolwarm\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a24afc",
   "metadata": {},
   "source": [
    "## Tests statistique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48540bb4",
   "metadata": {},
   "source": [
    "### my_pearson()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfa737bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_pearson(x, y):\n",
    "    \"\"\"\n",
    "    Calcule le coefficient de corrélation de Pearson entre deux variables x et y.\n",
    "    \n",
    "    Args:\n",
    "    - x, y (array-like): Deux tableaux de données unidimensionnels.\n",
    "    \n",
    "    Returns:\n",
    "    - pearson_coef (float): Le coefficient de corrélation de Pearson.\n",
    "    - p_value (float): La p-value associée au test d'hypothèse.\n",
    "    \n",
    "    Example:\n",
    "    my_pearson([1, 2, 3, 4, 5], [2, 4, 5, 7, 8])\n",
    "    \"\"\"\n",
    "    pearson_coef, p_value = pearsonr(x, y)\n",
    "    if p_value < 0.05:\n",
    "        print(\"P-value :\",p_value,\"L'hypothèse HO est vérifié\")\n",
    "    else :\n",
    "        print(\"P-value :\",p_value,\"L'Hypothèse H1 est vérifié\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1026616e",
   "metadata": {},
   "source": [
    "## Projection des individus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c08d4b2",
   "metadata": {},
   "source": [
    "### my_projection_individus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b482591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_projection_individus(   X_projected, \n",
    "                                x_y, \n",
    "                                pca=None, \n",
    "                                labels = None,\n",
    "                                clusters=None, \n",
    "                                alpha=1,\n",
    "                                figsize=[10,8], \n",
    "                                marker=\".\" ):\n",
    "    \"\"\"\n",
    "    Affiche la projection des individus\n",
    "\n",
    "    Positional arguments : \n",
    "    -------------------------------------\n",
    "    X_projected : np.array, pd.DataFrame, list of list : la matrice des points projetés\n",
    "    x_y : list ou tuple : le couple x,y des plans à afficher, exemple [0,1] pour F1, F2\n",
    "\n",
    "    Optional arguments : \n",
    "    -------------------------------------\n",
    "    pca : sklearn.decomposition.PCA : un objet PCA qui a été fit, cela nous permettra d'afficher la variance de chaque composante, default = None\n",
    "    labels : list ou tuple : les labels des individus à projeter, default = None\n",
    "    clusters : list ou tuple : la liste des clusters auquel appartient chaque individu, default = None\n",
    "    alpha : float in [0,1] : paramètre de transparence, 0=100% transparent, 1=0% transparent, default = 1\n",
    "    figsize : list ou tuple : couple width, height qui définit la taille de la figure en inches, default = [10,8] \n",
    "    marker : str : le type de marker utilisé pour représenter les individus, points croix etc etc, default = \".\"\n",
    "    \"\"\"\n",
    "\n",
    "    # Transforme X_projected en np.array\n",
    "    X_ = np.array(X_projected)\n",
    "\n",
    "    # On définit la forme de la figure si elle n'a pas été donnée\n",
    "    if not figsize: \n",
    "        figsize = (7,6)\n",
    "\n",
    "    # On gère les labels\n",
    "    if  labels is None : \n",
    "        labels = []\n",
    "    try : \n",
    "        len(labels)\n",
    "    except Exception as e : \n",
    "        raise e\n",
    "\n",
    "    # On vérifie la variable axis \n",
    "    if not len(x_y) ==2 : \n",
    "        raise AttributeError(\"2 axes sont demandées\")   \n",
    "    if max(x_y )>= X_.shape[1] : \n",
    "        raise AttributeError(\"la variable axis n'est pas bonne\")   \n",
    "\n",
    "    # on définit x et y \n",
    "    x, y = x_y\n",
    "\n",
    "    # Initialisation de la figure       \n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "\n",
    "    # On vérifie s'il y a des clusters ou non\n",
    "    c = None if clusters is None else clusters\n",
    " \n",
    "    # Les points    \n",
    "    # plt.scatter(   X_[:, x], X_[:, y], alpha=alpha, \n",
    "    #                     c=c, cmap=\"Set1\", marker=marker)\n",
    "    sns.scatterplot(data=None, x=X_[:, x], y=X_[:, y], hue=c)\n",
    "\n",
    "    # Si la variable pca a été fournie, on peut calculer le % de variance de chaque axe \n",
    "    if pca : \n",
    "        v1 = str(round(100*pca.explained_variance_ratio_[x]))  + \" %\"\n",
    "        v2 = str(round(100*pca.explained_variance_ratio_[y]))  + \" %\"\n",
    "    else : \n",
    "        v1=v2= ''\n",
    "\n",
    "    # Nom des axes, avec le pourcentage d'inertie expliqué\n",
    "    ax.set_xlabel(f'F{x+1} {v1}')\n",
    "    ax.set_ylabel(f'F{y+1} {v2}')\n",
    "\n",
    "    # Valeur x max et y max\n",
    "    x_max = np.abs(X_[:, x]).max() *1.1\n",
    "    y_max = np.abs(X_[:, y]).max() *1.1\n",
    "\n",
    "    # On borne x et y \n",
    "    ax.set_xlim(left=-x_max, right=x_max)\n",
    "    ax.set_ylim(bottom= -y_max, top=y_max)\n",
    "\n",
    "    # Affichage des lignes horizontales et verticales\n",
    "    plt.plot([-x_max, x_max], [0, 0], color='grey', alpha=0.8)\n",
    "    plt.plot([0,0], [-y_max, y_max], color='grey', alpha=0.8)\n",
    "\n",
    "    # Affichage des labels des points\n",
    "    if len(labels) : \n",
    "        for i,(_x,_y) in enumerate(X_[:,[x,y]]):\n",
    "            plt.text(_x, _y+0.05, labels[i], fontsize='14', ha='center',va='center') \n",
    "\n",
    "    # Titre et display\n",
    "    plt.title(f\"Projection des individus (sur F{x+1} et F{y+1})\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3163cebc",
   "metadata": {},
   "source": [
    "## Analyse en Composante Principales (ACP ou PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c8b43c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1de05c3e",
   "metadata": {},
   "source": [
    "### my_pca_coude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7843d9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_pca_coude(data, n_components_max=None):\n",
    "    \"\"\"\n",
    "    \n",
    "    Execute la méthode du coude pour déterminer le nombre idéal de composantes principales d'une ACP.\n",
    "\n",
    "    Paramètre:\n",
    "    -----------\n",
    "    data : pandas.DataFrame\n",
    "        Les données à analyser\n",
    "    n_components_max : int, optionnel\n",
    "        Le nombre maximum de composantes principales à tester. Aucune limite par défaut (i.e. n_components_max=len(data.columns)-1).\n",
    "    \n",
    "    Sortie:\n",
    "    --------\n",
    "        Le graphique coude.\n",
    "    \"\"\"\n",
    "    # on définit le nombre de composantes principales à tester. \n",
    "    if n_components_max is None:\n",
    "        # Si None, on selectionne le nombre de colonne du dataframe -1\n",
    "        n_components_max = len(data.columns) - 1\n",
    "\n",
    "    # On initialise notre PCA    \n",
    "    pca = PCA(n_components=n_components_max)\n",
    "    # On entraine notre PCA\n",
    "    pca.fit(data)\n",
    "    # On récupère la variance de chaque composantes\n",
    "    explained_var = pca.explained_variance_ratio_\n",
    "    # On additionne chaque variance pour avoir la somme cumulée\n",
    "    cumsum_var = np.cumsum(explained_var)\n",
    "\n",
    "    # On gènère notre graphique\n",
    "    plt.plot(range(1, n_components_max+1), cumsum_var, 'bo-')\n",
    "    plt.xlabel('Nombre de composantes principales')\n",
    "    plt.ylabel('Somme de la variance')\n",
    "    plt.title('Méthode du coude pour ACP')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c0bd82",
   "metadata": {},
   "source": [
    "### my_pca()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "907853d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_pca(n_components, X_scaled, x_y, scaling=False):\n",
    "    \"\"\"\n",
    "    Effectue une Analyse en Composantes Principales (PCA) sur les données.\n",
    "    \n",
    "    Args:\n",
    "    - n_components (int): Le nombre de composantes principales à extraire.\n",
    "    - X_scaled (pandas DataFrame): Le dataframe contenant les données à analyser.\n",
    "    - x_y (tuple): Un tuple contenant les index de deux composantes à comparer.\n",
    "    - scaling (bool): Si True, standardise les données avant la PCA.\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \n",
    "    Example:\n",
    "    my_pca(3, df, (0, 1), True)\n",
    "    \"\"\"\n",
    "    # Définition des données\n",
    "    X = X_scaled.values\n",
    "    # Définition des noms des index (lignes) de notre jeu de données\n",
    "    names = X_scaled.index\n",
    "    # Définition des noms des colonnes de notre jeu de données\n",
    "    features = X_scaled.columns\n",
    "    \n",
    "    if scaling is True:\n",
    "        #Scaling\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # PCA\n",
    "    pca = decomposition.PCA(n_components=n_components)\n",
    "    # On entraine nos données\n",
    "    pca.fit(X_scaled)\n",
    "    # On calcule le ratio de la variance\n",
    "    pca.explained_variance_ratio_\n",
    "    scree = (pca.explained_variance_ratio_*100).round(2)\n",
    "    # En cumulé\n",
    "    scree_cum = scree.cumsum().round()\n",
    "    print(\"Explained Variance Ratio :\",scree_cum)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Affichage graphique de l'Eboulis des valeurs propres\n",
    "    x_list = range(1, n_components+1)\n",
    "    list(x_list)\n",
    "    \n",
    "    plt.bar(x_list, scree)\n",
    "    plt.plot(x_list, scree_cum,c=\"red\",marker='o')\n",
    "    plt.xlabel(\"rang de l'axe d'inertie\")\n",
    "    plt.ylabel(\"pourcentage d'inertie\")\n",
    "    plt.title(\"Eboulis des valeurs propres\")\n",
    "    plt.show(block=False)\n",
    "    \n",
    "    # Définition des composantes principales\n",
    "    pcs = pca.components_\n",
    "    pcs = pd.DataFrame(pcs)\n",
    "    pcs.columns = features\n",
    "    pcs.index = [f\"F{i}\" for i in x_list]\n",
    "    pcs.round(2)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(20, 6))\n",
    "    sns.heatmap(pcs.T, vmin=-1, vmax=1, annot=True, cmap=\"coolwarm\", fmt=\"0.2f\")\n",
    "    plt.title(\"Test\")\n",
    "    plt.xlabel(\"Composante\")\n",
    "    \n",
    "    \n",
    "    # Correlation graph\n",
    "    \n",
    "    # Extrait x et y \n",
    "    x,y=x_y\n",
    "\n",
    "    # Taille de l'image (en inches)\n",
    "    fig, ax = plt.subplots(figsize=(10, 9))\n",
    "\n",
    "    # Pour chaque composante : \n",
    "    for i in range(0, pca.components_.shape[1]):\n",
    "\n",
    "        # Les flèches\n",
    "        ax.arrow(0,0, \n",
    "                pca.components_[x, i],  \n",
    "                pca.components_[y, i],  \n",
    "                head_width=0.07,\n",
    "                head_length=0.07, \n",
    "                width=0.02, )\n",
    "\n",
    "        # Les labels\n",
    "        plt.text(pca.components_[x, i] + 0.05,\n",
    "                pca.components_[y, i] + 0.05,\n",
    "                features[i])\n",
    "        \n",
    "    # Affichage des lignes horizontales et verticales\n",
    "    plt.plot([-1, 1], [0, 0], color='grey', ls='--')\n",
    "    plt.plot([0, 0], [-1, 1], color='grey', ls='--')\n",
    "\n",
    "    # Nom des axes, avec le pourcentage d'inertie expliqué\n",
    "    plt.xlabel('F{} ({}%)'.format(x+1, round(100*pca.explained_variance_ratio_[x],1)))\n",
    "    plt.ylabel('F{} ({}%)'.format(y+1, round(100*pca.explained_variance_ratio_[y],1)))\n",
    "\n",
    "    # Titre du graphique\n",
    "    plt.title(\"Cercle des corrélations (F{} et F{})\".format(x+1, y+1))\n",
    "\n",
    "    # Le cercle \n",
    "    an = np.linspace(0, 2 * np.pi, 100)\n",
    "    plt.plot(np.cos(an), np.sin(an))  # Add a unit circle for scale\n",
    "\n",
    "    # Axes et display\n",
    "    plt.axis('equal')\n",
    "    plt.show(block=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c2b140",
   "metadata": {},
   "source": [
    "## KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e293bf",
   "metadata": {},
   "source": [
    "### my_kmeans_coude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba86471f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_kmeans_coude(data, cluster_max=10):\n",
    "    \"\"\"\n",
    "    Utilise la méthode du coude pour déterminer le nombre optimal de clusters à utiliser dans l'algorithme K-Means.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : DataFrame\n",
    "        Le jeu de données à analyser.\n",
    "    cluster_max : int, optional\n",
    "        Le nombre maximal de clusters à tester.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    int\n",
    "        Le nombre de clusters optimal déterminé par la méthode du coude.\n",
    "    \"\"\"       \n",
    "        \n",
    "    inertia = []\n",
    "    k_list = range(1, cluster_max)\n",
    "    for k in k_list :\n",
    "        kmeans = KMeans(n_clusters=k)\n",
    "        kmeans.fit(data)\n",
    "        inertia.append(kmeans.inertia_)\n",
    "    #Graphique\n",
    "    fig, ax = plt.subplots(1,1,figsize=(12,6))\n",
    "    ax.set_ylabel(\"inertia\")\n",
    "    ax.set_xlabel(\"n_cluster\")\n",
    "    ax.plot(k_list, inertia)\n",
    "    plt.show()\n",
    "    \n",
    "    # Trouver l'indice du nombre de clusters optimal\n",
    "    optimal_index = np.argmin(np.diff(inertia)) + 1\n",
    "    \n",
    "    # Récupérer le nombre de clusters optimal\n",
    "    n_clusters = optimal_index + 1\n",
    "    print(\"D'après la méthode du coude, le nombre de cluster optimal est :\",n_clusters)\n",
    "    return n_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80418d0",
   "metadata": {},
   "source": [
    "### my_silhouette_method()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31857309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_silhouette_method(data, max_clusters):\n",
    "    \"\"\"\n",
    "    Cette fonction effectue une analyse de silhouette pour déterminer le nombre optimal de clusters à utiliser pour\n",
    "    la méthode de clustering de KMeans.\n",
    "\n",
    "    :param data: DataFrame avec les données à utiliser pour l'analyse\n",
    "    :param max_clusters: Nombre maximum de clusters à tester\n",
    "    :return: Le modèle KMeans avec le nombre optimal de clusters selon l'analyse de silhouette\n",
    "    \"\"\"\n",
    "    \n",
    "          \n",
    "\n",
    "    # Liste pour stocker les scores de silhouette\n",
    "    silhouette_scores = []\n",
    "\n",
    "    # Boucle pour tester différents nombres de clusters\n",
    "    for num_clusters in range(2, max_clusters+1):\n",
    "        kmeans = KMeans(n_clusters=num_clusters, init='k-means++', max_iter=100, n_init=10, random_state=42)\n",
    "        kmeans.fit(data)\n",
    "        cluster_labels = kmeans.labels_\n",
    "\n",
    "        # Calculer le score de silhouette pour chaque point\n",
    "        silhouette_avg = silhouette_score(data, cluster_labels)\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "    # Trouver le nombre optimal de clusters en se basant sur le score de silhouette le plus élevé\n",
    "    optimal_num_clusters = silhouette_scores.index(max(silhouette_scores)) + 2\n",
    "\n",
    "    # Afficher le nombre optimal de cluster\n",
    "    print(\"Le nombre optimal de cluster est :\", optimal_num_clusters)\n",
    "\n",
    "    # Entraîner le modèle KMeans avec le nombre optimal de clusters et retourner le modèle\n",
    "    kmeans = KMeans(n_clusters=optimal_num_clusters, init='k-means++', max_iter=100, n_init=10, random_state=42)\n",
    "    kmeans.fit(data)\n",
    "    \n",
    "    plt.plot(range(2, max_clusters+1), silhouette_scores, marker='o')\n",
    "    plt.xlabel('Nombre de cluster')\n",
    "    plt.ylabel('Score Silhouette')\n",
    "    plt.show()\n",
    "    \n",
    "    return kmeans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca1ac79",
   "metadata": {},
   "source": [
    "### my_kmeans()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c99e58c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_kmeans(data, n_clusters):\n",
    "    \n",
    "    \n",
    "    # On initialise le K-Means sur nos données\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    # On entraine nos données\n",
    "    kmeans.fit(data)\n",
    "    # On récupère le numéro de chaque cluster dans une variable\n",
    "    labels = kmeans.labels_\n",
    "     \n",
    "    \n",
    "    # PCA\n",
    "    # On initialise le PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    # On entraine nos données\n",
    "    pca.fit(data)\n",
    "    # On récupère le score de chaque composantes\n",
    "    pca_scores = pca.transform(data)\n",
    "    # on stock ces information dans un DataFrame\n",
    "    pca_df = pd.DataFrame(pca_scores, columns=['PC1', 'PC2'])\n",
    "    \n",
    "    # Graphique\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    sns.scatterplot(x=\"PC1\", y=\"PC2\", hue=labels, palette=sns.color_palette(\"hls\", n_clusters), data=pca_df, ax=ax)\n",
    "    # On stock nos centroid dans une variable pour pouvoir la visualier plus tard.\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    # On affiche un Dataframe avec chaque centroid pour chaque variable\n",
    "    display(pd.DataFrame(centroids, columns=data.columns))\n",
    "    # On affiche les centroid sur le graphique\n",
    "    ax.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='s', s=100)\n",
    "    ax.set_title(\"Projection des individus\")\n",
    "    ax.set_xlabel(\"PC1 ({:.2f}%)\".format(pca.explained_variance_ratio_[0] * 100))\n",
    "    ax.set_ylabel(\"PC2 ({:.2f}%)\".format(pca.explained_variance_ratio_[1] * 100))\n",
    "    plt.show()\n",
    "    # On ajouter une nouvelle colonnes dans notre DataFrame initiale avec le numéro de chaque cluster.\n",
    "    data[\"cluster\"] = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9108a4",
   "metadata": {},
   "source": [
    "### my_all_kmeans()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a6226bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_all_kmeans(data, cluster_max=10, graphique=False):\n",
    "    \"\"\"\n",
    "    Cette fonction applique la méthode du coude pour trouver le nombre de clusters optimal, puis effectue l'algorithme de K-means sur les données passées en paramètre avec le nombre de clusters optimal trouvé. Elle prend en entrée les données à traiter, le nombre maximum de clusters à considérer, ainsi qu'un paramètre optionnel graphique qui, s'il est True, permet de visualiser les clusters trouvés à travers un pairplot.\n",
    "    \n",
    "    Args:\n",
    "        data : Les données à traiter.\n",
    "        cluster_max : Le nombre maximum de clusters à considérer.\n",
    "        graphique : Si True, permet de visualiser les clusters trouvés à travers un pairplot.\n",
    "    \"\"\"\n",
    "    #Méthode du coude\n",
    "    inertia = []\n",
    "    k_list = range(1, cluster_max)\n",
    "    for k in k_list :\n",
    "        kmeans = KMeans(n_clusters=k)\n",
    "        kmeans.fit(data)\n",
    "        inertia.append(kmeans.inertia_)\n",
    "    #Graphique\n",
    "    fig, ax = plt.subplots(1,1,figsize=(12,6))\n",
    "    ax.set_ylabel(\"Inertie\")\n",
    "    ax.set_xlabel(\"Nombre de clusters\")\n",
    "    ax.plot(k_list, inertia)\n",
    "    plt.show()\n",
    "    \n",
    "    # Trouver l'indice du nombre de clusters optimal\n",
    "    optimal_index = np.argmin(np.diff(inertia)) + 1\n",
    "    \n",
    "    # Récupérer le nombre de clusters optimal\n",
    "    n_clusters = optimal_index + 1\n",
    "    print(\"D'après la méthode du coude, le nombre de cluster optimal est :\",n_clusters)\n",
    "    \n",
    "\n",
    "    if graphique is True:\n",
    "        sns.pairplot(data, hue=\"cluster\")\n",
    "        plt.figure(figsize=(25,25))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aa1d1a",
   "metadata": {},
   "source": [
    "## Dendrogramme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d3aadd",
   "metadata": {},
   "source": [
    "### my_dendrogram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f24b086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_dendrogram(data, nombre_cluster, methode=\"ward\"):\n",
    "    \"\"\"\n",
    "    Cette fonction effectue l'algorithme de clustering hiérarchique sur les données passées en paramètre et affiche le dendrogramme correspondant. Elle prend en entrée les données à traiter, le nombre de clusters désiré, ainsi qu'un paramètre optionnel methode qui permet de spécifier la méthode à utiliser pour le clustering.\n",
    "    \n",
    "    Args:\n",
    "        data : Les données à traiter.\n",
    "        nombre_cluster : Le nombre de clusters désiré.\n",
    "        methode : La méthode à utiliser pour le clustering.\n",
    "    \"\"\"\n",
    "    Z = linkage(data, method=methode)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(90, 40))\n",
    "\n",
    "    _ = dendrogram(Z, p=nombre_cluster, truncate_mode=\"lastp\", ax=ax,labels=data.index)\n",
    "\n",
    "    # Récupération des clusters\n",
    "    clusters = fcluster(Z, t=nombre_cluster, criterion='maxclust')\n",
    "    \n",
    "    #Index trié des groupes\n",
    "    idg = np.argsort(clusters)\n",
    "    \n",
    "    #Affichage des pays selon leurs groupes\n",
    "    df_groupes_cah = pd.DataFrame(data.index[idg], clusters[idg]).reset_index()\n",
    "    df_groupes_cah = df_groupes_cah.rename(columns={'index':'Groupe'})\n",
    "    \n",
    "    #Jointure interne nécessaire pour parvenir à agréger nos données\n",
    "    df_new = pd.merge(data, df_groupes_cah, on='Zone')\n",
    "    \n",
    "    # Affichage des clusters\n",
    "    for i in range(1, nombre_cluster + 1):\n",
    "        print(f\"Cluster {i}: {data.index[clusters == i].tolist()}\")\n",
    "\n",
    "    plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "    plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
    "    plt.ylabel(\"Distance.\")\n",
    "    plt.xticks(rotation=90)\n",
    "    ax.tick_params(axis='x', labelsize=30)\n",
    "    plt.show()\n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b475e6f1",
   "metadata": {},
   "source": [
    "## PCA + Projection des individus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f90bbf6",
   "metadata": {},
   "source": [
    "### my_pca_proj()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76137cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_pca_proj(data,\n",
    "                n_components,  \n",
    "                x_y,\n",
    "                center=None,\n",
    "                projection=False,\n",
    "                sup_columns=None,\n",
    "                sup_row=None,\n",
    "                scaling=False,\n",
    "                type_scaling=\"standard\",\n",
    "                labels = None, \n",
    "                clusters=None,\n",
    "                transform_data=None,\n",
    "                alpha=1,\n",
    "                figsize=[10,8], \n",
    "                marker=\".\" ):\n",
    "    \n",
    "    \"\"\"\n",
    "    Cette fonction effectue une analyse en composantes principales (PCA) sur les données passées en paramètre et projette les données dans l'espace réduit défini par les premières composantes principales. Elle prend en entrée les données à traiter ainsi que plusieurs paramètres optionnels pour spécifier des options de pré-traitement des données (suppression de colonnes ou de lignes, scaling), le nombre de composantes principales souhaité, le type de scaling à appliquer, des labels ou des clusters si on souhaite colorer les points du graphique obtenu, et des options d'affichage.\n",
    "    \n",
    "    Args:\n",
    "        data : Les données à traiter.\n",
    "        n_components : Le nombre de composantes principales souhaité.\n",
    "        x_y : Les deux composantes principales à afficher sur le graphique.\n",
    "        sup_columns : Les noms des colonnes à supprimer (optionnel).\n",
    "        sup_row : Les noms des lignes à supprimer (optionnel).\n",
    "        scaling : Si True, effectue un scaling sur les données (optionnel).\n",
    "        type_scaling : Le type de scaling à appliquer (standard ou robust) (optionnel).\n",
    "        labels : Les labels à afficher sur le graphique (optionnel).\n",
    "        clusters : Les clusters à afficher sur le graphique (optionnel).\n",
    "        transform_data: option pour la transformation des données, possible choix : 'log' ou 'sqrt', de type str ou None (optionnel).\n",
    "        alpha: niveau de transparence des points sur le graphique, de type float(optionnel).\n",
    "        figsize: taille de la figure à afficher, de type list(optionnel).\n",
    "        marker: forme des points à afficher sur le graphique, de type str (optionnel).\n",
    "\n",
    "        return: None (la fonction affiche les résultats graphiques directement\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    \n",
    "                                                    # My_pca\n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Si le paramètre sup_columns n'est pas null, alors il faut supprimer les colonnes en paramètre\n",
    "    if sup_columns is not None:\n",
    "        data = data.drop(sup_columns, axis=1)\n",
    "    \n",
    "    # Si le paramètre sup_row n'est pas null, alors il faut supprimer les colonnes en paramètre\n",
    "    if sup_row is not None:\n",
    "        data = data.drop(sup_row, axis=0)\n",
    "            \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Vérifie le type de transformation de données souhaité\n",
    "    # Log() -> Voir log négatif : valeur absolu -> log -> repasser en négatif\n",
    "    if transform_data == \"log\":\n",
    "        for i in data.all():\n",
    "            if (i == 0) or (i < 0):\n",
    "                return print(\"Log() impossible : Valeur égale ou inférieur à 0 !\")\n",
    "            else:\n",
    "                data = np.log(data)\n",
    "    # Racine carré            \n",
    "    elif transform_data == \"sqrt\":\n",
    "        for i in data.columns:\n",
    "            data[i] = np.sqrt(data[[i]])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Définition des données\n",
    "    X = data.values\n",
    "    # Définition des noms des index (lignes) de notre jeu de données\n",
    "    names = data.index\n",
    "    # Définition des noms des colonnes de notre jeu de données\n",
    "    features = data.columns\n",
    "    \n",
    "    # Vérifie le type de scaling souhaité\n",
    "    #Standard Scaling\n",
    "    if scaling is True:\n",
    "        if type_scaling ==\"standard\":\n",
    "            # Standard Scaling\n",
    "            scaler = preprocessing.StandardScaler()\n",
    "            X_scaled = scaler.fit_transform(X)\n",
    "    #Robust Scaling\n",
    "        elif type_scaling==\"robust\":\n",
    "            # Robust Scaling\n",
    "            scaler = preprocessing.RobustScaler()\n",
    "            X_scaled = scaler.fit_transform(X)\n",
    "    # Données non scalé\n",
    "    else:\n",
    "        X_scaled = X\n",
    "    \n",
    "    # PCA\n",
    "    pca = decomposition.PCA(n_components=n_components)\n",
    "    # On entraine nos données\n",
    "    pca.fit(X_scaled)\n",
    "    # On calcule le ratio de la variance\n",
    "    pca.explained_variance_ratio_\n",
    "    scree = (pca.explained_variance_ratio_*100).round(2)\n",
    "    # En cumulé\n",
    "    scree_cum = scree.cumsum().round()\n",
    "    print(\"Explained Variance Ratio :\",scree_cum)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Affichage graphique de l'Eboulis des valeurs propres\n",
    "    x_list = range(1, n_components+1)\n",
    "    list(x_list)\n",
    "    \n",
    "    plt.bar(x_list, scree)\n",
    "    plt.plot(x_list, scree_cum,c=\"red\",marker='o')\n",
    "    plt.xlabel(\"rang de l'axe d'inertie\")\n",
    "    plt.ylabel(\"pourcentage d'inertie\")\n",
    "    plt.title(\"Eboulis des valeurs propres\")\n",
    "    plt.show(block=False)\n",
    "    \n",
    "    # Définition des composantes principales\n",
    "    pcs = pca.components_\n",
    "    pcs = pd.DataFrame(pcs)\n",
    "    pcs.columns = features\n",
    "    pcs.index = [f\"F{i}\" for i in x_list]\n",
    "    pcs.round(2)\n",
    "  \n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(20, 6))\n",
    "    sns.heatmap(pcs.T, vmin=-1, vmax=1, annot=True, cmap=\"coolwarm\", fmt=\"0.2f\")\n",
    "    plt.title(\"Tableau de corrélation ACP\")\n",
    "    plt.xlabel(\"Composante\")\n",
    "     #------------------------------------------------------------------------------------------------\n",
    "    \n",
    "                                                # Correlation graph\n",
    "        \n",
    "     #------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Extrait x et y \n",
    "    x,y=x_y\n",
    "\n",
    "    # Taille de l'image (en inches)\n",
    "    fig, ax = plt.subplots(figsize=(10, 9))\n",
    "\n",
    "    # Pour chaque composante : \n",
    "    for i in range(0, pca.components_.shape[1]):\n",
    "\n",
    "        # Les flèches\n",
    "        ax.arrow(0,0, \n",
    "                pca.components_[x, i],  \n",
    "                pca.components_[y, i],  \n",
    "                head_width=0.07,\n",
    "                head_length=0.07, \n",
    "                width=0.02, )\n",
    "\n",
    "        # Les labels\n",
    "        plt.text(pca.components_[x, i] + 0.05,\n",
    "                pca.components_[y, i] + 0.05,\n",
    "                features[i])\n",
    "        \n",
    "    # Affichage des lignes horizontales et verticales\n",
    "    plt.plot([-1, 1], [0, 0], color='grey', ls='--')\n",
    "    plt.plot([0, 0], [-1, 1], color='grey', ls='--')\n",
    "\n",
    "    # Nom des axes, avec le pourcentage d'inertie expliqué\n",
    "    plt.xlabel('F{} ({}%)'.format(x+1, round(100*pca.explained_variance_ratio_[x],1)))\n",
    "    plt.ylabel('F{} ({}%)'.format(y+1, round(100*pca.explained_variance_ratio_[y],1)))\n",
    "\n",
    "    # Titre du graphique\n",
    "    plt.title(\"Cercle des corrélations (F{} et F{})\".format(x+1, y+1))\n",
    "\n",
    "    # Le cercle \n",
    "    an = np.linspace(0, 2 * np.pi, 100)\n",
    "    plt.plot(np.cos(an), np.sin(an))  # Add a unit circle for scale\n",
    "\n",
    "    # Axes et display\n",
    "    plt.axis('equal')\n",
    "    plt.show(block=False)\n",
    "    \n",
    "\n",
    "    #------------------------------------------------------------------------------------------------\n",
    "    \n",
    "                                        # my_projection_individus\n",
    "        \n",
    "    #------------------------------------------------------------------------------------------------\n",
    "    if projection == True:\n",
    "        # On projette nos données\n",
    "        X_projected = pca.transform(X_scaled)\n",
    "\n",
    "        # Transforme X_projected en np.array\n",
    "        X_ = np.array(X_projected)\n",
    "\n",
    "        # On définit la forme de la figure si elle n'a pas été donnée\n",
    "        if not figsize: \n",
    "            figsize = (7,6)\n",
    "\n",
    "        # On gère les labels\n",
    "        if  labels is None : \n",
    "            labels = []\n",
    "        try : \n",
    "            len(labels)\n",
    "        except Exception as e : \n",
    "            raise e\n",
    "\n",
    "        # On vérifie la variable axis \n",
    "        if not len(x_y) ==2 : \n",
    "            raise AttributeError(\"2 axes sont demandées\")   \n",
    "        if max(x_y )>= X_.shape[1] : \n",
    "            raise AttributeError(\"la variable axis n'est pas bonne\")   \n",
    "\n",
    "\n",
    "        # Initialisation de la figure       \n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10,8))\n",
    "\n",
    "        # On vérifie s'il y a des clusters ou non\n",
    "        cl = None if clusters is None else clusters\n",
    "\n",
    "                      \n",
    "\n",
    "\n",
    "        display(df)\n",
    "        display(center)\n",
    "        sns.scatterplot(data=None, x=X_[:, x], y=X_[:, y], hue=cl)\n",
    "        sns.scatterplot(center[:, x], center[:, y], c='red', marker='s', s='100000')\n",
    "\n",
    "\n",
    "\n",
    "        # Si la variable pca a été fournie, on peut calculer le % de variance de chaque axe \n",
    "        if pca : \n",
    "            v1 = str(round(100*pca.explained_variance_ratio_[x]))  + \" %\"\n",
    "            v2 = str(round(100*pca.explained_variance_ratio_[y]))  + \" %\"\n",
    "        else : \n",
    "            v1=v2= ''\n",
    "\n",
    "        # Nom des axes, avec le pourcentage d'inertie expliqué\n",
    "        ax.set_xlabel(f'F{x+1} {v1}')\n",
    "        ax.set_ylabel(f'F{y+1} {v2}')\n",
    "\n",
    "        # Valeur x max et y max\n",
    "        x_max = np.abs(X_[:, x]).max() *1.1\n",
    "        y_max = np.abs(X_[:, y]).max() *1.1\n",
    "\n",
    "        # On borne x et y \n",
    "        ax.set_xlim(left=-x_max, right=x_max)\n",
    "        ax.set_ylim(bottom= -y_max, top=y_max)\n",
    "\n",
    "        # Affichage des lignes horizontales et verticales\n",
    "        plt.plot([-x_max, x_max], [0, 0], color='grey', alpha=0.8)\n",
    "        plt.plot([0,0], [-y_max, y_max], color='grey', alpha=0.8)\n",
    "\n",
    "        # Affichage des labels des points\n",
    "        if len(labels) : \n",
    "            for i,(_x,_y) in enumerate(X_[:,[x,y]]):\n",
    "                plt.text(_x, _y+0.05, labels[i], fontsize='9', ha='center',va='center') \n",
    "\n",
    "        # Titre et display\n",
    "        plt.title(f\"Projection des individus (sur F{x+1} et F{y+1})\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d863591f",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699afa0d",
   "metadata": {},
   "source": [
    "## my_backward_selected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f05deaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_backward_selected(data, response):\n",
    "    \"\"\"Linear model designed by backward selection.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pandas DataFrame with all possible predictors and response\n",
    "\n",
    "    response: string, name of response column in data\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    model: an \"optimal\" fitted statsmodels linear model\n",
    "           with an intercept\n",
    "           selected by backward selection\n",
    "           evaluated by parameters p-value\n",
    "    \"\"\"\n",
    "    remaining = set(data._get_numeric_data().columns)\n",
    "    if response in remaining:\n",
    "        remaining.remove(response)\n",
    "    cond = True\n",
    "\n",
    "    while remaining and cond:\n",
    "        formula = \"{} ~ {} + 1\".format(response,' + '.join(remaining))\n",
    "        print('_______________________________')\n",
    "        print(formula)\n",
    "        model = smf.ols(formula, data).fit()\n",
    "        score = model.pvalues[1:]\n",
    "        toRemove = score[score == score.max()]\n",
    "        if toRemove.values > 0.05:\n",
    "            print('remove', toRemove.index[0], '(p-value :', round(toRemove.values[0],3), ')')\n",
    "            remaining.remove(toRemove.index[0])\n",
    "        else:\n",
    "            cond = False\n",
    "            print('is the final model !')\n",
    "        print('')\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8ddfe0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Sommaire",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "638px",
    "left": "1559px",
    "top": "110px",
    "width": "322px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
